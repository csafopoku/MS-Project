# -*- coding: utf-8 -*-
"""Group 5_Mid-Semester Project: Sports Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eWc_wYJ_2pku6b98aWs-QTUclvFwp0UN
"""

# code for importing the necessary libraries and modules for machine learning and data analysis.

import os
import json
import pickle
import random
import sklearn
import requests
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.svm import SVC
from google.colab import drive
import matplotlib.pyplot as plt
from sklearn import linear_model
from sklearn import tree, metrics
from sklearn.metrics import r2_score
from sklearn.impute import SimpleImputer
from flask import Flask,request, jsonify
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import mean_absolute_error, make_scorer
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error


drive.mount('/content/drive')

# Loading player 21 data from a CSV file
players21_data=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

# Loading player 22 data from a CSV file
players22_data=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

"""DATA PREPROCESSING

---


"""

# Generating summary statistics for the player 21 data.
players21_data.describe()

# Displaying the first few rows of player data.
players21_data.head()

# Removing specific columns from the 'players21_data' DataFrame as they are derived from existing features and have no bearing on the rating.
players21_data=players21_data.drop(['sofifa_id','ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram','lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb','lb', 'lcb', 'cb', 'rcb', 'rb', 'gk','player_url','player_face_url','player_positions','club_logo_url','club_flag_url','club_joined','dob','nation_flag_url','short_name', 'long_name'],axis=1)

# Removing specific columns from the 'players22_data' DataFrame as they are derived from existing features and have no bearing on the rating.
players22_data=players22_data.drop(['sofifa_id','ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram','lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb','lb', 'lcb', 'cb', 'rcb', 'rb', 'gk','player_url','player_face_url','player_positions','club_logo_url','club_flag_url','club_joined','dob','nation_flag_url','short_name', 'long_name'],axis=1)

# Dropping columns that have 30% or more of their values as null to reduce bias
biased_values =  0.30 * len(players21_data)
players21_data = players21_data.loc[:, players21_data.isna().sum() < biased_values]

# Dropping columns from 'players22_data' that are not present in 'players21_data'.
dropped_columns = []
for column_name in players22_data.columns:
    if column_name not in players21_data.columns:
        dropped_columns.append(column_name)

players22_data=players22_data.drop(dropped_columns,axis=1)

# Handling missing values by filling numerical columns with their mean.
numerical_columns = ["overall", "potential", "value_eur", "wage_eur", "age", "height_cm", "weight_kg", "club_team_id", "league_level", "club_jersey_number", "club_contract_valid_until", "nationality_id", "weak_foot", "skill_moves", "international_reputation", "release_clause_eur", "pace", "shooting", "passing", "dribbling", "defending", "physic", "attacking_crossing", "attacking_finishing", "attacking_heading_accuracy", "attacking_short_passing", "attacking_volleys", "skill_dribbling", "skill_curve", "skill_fk_accuracy", "skill_long_passing", "skill_ball_control", "movement_acceleration", "movement_sprint_speed"
,"movement_agility", "movement_reactions", "movement_balance", "power_shot_power", "power_jumping", "power_stamina", "power_strength", "power_long_shots", "mentality_aggression", "mentality_interceptions", "mentality_positioning", "mentality_vision", "mentality_penalties", "mentality_composure", "defending_marking_awareness", "defending_standing_tackle", "defending_sliding_tackle", "goalkeeping_diving", "goalkeeping_handling", "goalkeeping_kicking", "goalkeeping_positioning", "goalkeeping_reflexes"]
categorical_columns=['club_name','league_name','real_face','club_position','nationality_name','preferred_foot','work_rate','body_type']
players21_data[numerical_columns] = players21_data[numerical_columns].apply(lambda col: col.fillna(np.mean(col)))
players22_data[numerical_columns] = players22_data[numerical_columns].apply(lambda col: col.fillna(np.mean(col)))

# Impute missing values with mode for players21_data
for column in categorical_columns:
    mode = players21_data[column].mode()[0]
    players21_data[column].fillna(mode, inplace=True)

# Impute missing values with mode for players22_data
for column in categorical_columns:
    mode = players22_data[column].mode()[0]
    players22_data[column].fillna(mode, inplace=True)

# Filling missing values in 'players22_data' numerical columns with their means.
players22_data[numerical_columns] = players22_data[numerical_columns].apply(lambda col: col.fillna(np.mean(col)))

"""FEATURE ENGINEERING

---


"""

# One-hot encoding categorical columns in 'players21_data'.
one_hot_encoded = pd.get_dummies(players21_data[categorical_columns], columns=categorical_columns)
players21_data = players21_data.drop(columns=categorical_columns)
players21_data = pd.concat([players21_data, one_hot_encoded], axis=1)

# One-hot encoding categorical columns in 'players22_data'.
one_hot_encoded_2 = pd.get_dummies(players22_data[categorical_columns], columns=categorical_columns)
players22_data = players22_data.drop(columns=categorical_columns)
players22_data = pd.concat([players22_data, one_hot_encoded_2], axis=1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(players21_data.drop('overall', axis=1), players21_data['overall'], test_size=0.2, random_state=0)

# Create a Random Forest Regressor
forest = RandomForestRegressor(n_estimators=250, random_state=0)

# Fit the model on the training data
forest.fit(X_train, y_train)

# Get feature importances
feature_importances = forest.feature_importances_

# Get feature names
feature_names = X_train.columns

# Create a dictionary of feature names and their importances
feature_importance_dict = dict(zip(feature_names, feature_importances))

# Sort features by importance (from highest to lowest)
sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# Select the top 13 features based on importance
selected_features = sorted_features[:18]

# Extract the names of the selected features
selected_feature_names = [feature[0] for feature in selected_features]

# Dropping of columns that are categorical and encoded
selected_feature_names.remove('nationality_name_New Zealand')
selected_feature_names.remove('league_name_Campeonato Brasileiro SÃ©rie A')
selected_feature_names.remove('physic')

players21_data[selected_feature_names]

# Selecting specific features for 'X' and 'X1'.
X = players21_data[selected_feature_names]
X_22 = players22_data[selected_feature_names]

# Scaling feature data for 'X' and 'X1', and creating new DataFrames.
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

scaled_players21_data = pd.concat([X_scaled, players21_data['overall']], axis=1)

X_22_scaled = pd.DataFrame(scaler.fit_transform(X_22), columns=X_22.columns)
scaled_players22_data = pd.concat([X_22_scaled, players22_data['overall']], axis=1)

"""TRAINING

---


"""

# Assigning scaled feature data and splitting it into training and testing sets.
X=X_scaled
y=scaled_players21_data['overall']

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2,  random_state=42)

"""RANDOM FOREST"""

# Training a Random Forest Regressor model and making predictions.
rf=RandomForestRegressor()
rf.fit(X_train, Y_train)
y_pred_rf =rf.predict(X_test)
print(y_pred_rf)

# Testing the Random Forest Regressor
mae_rf = mean_absolute_error(Y_test, y_pred_rf)
print(mae_rf)

"""XGBOOST
---



"""

# Create a DMatrix, train an XGBoost Regressor, make predictions, and return the predictions.
data_dmatrix = xgb.DMatrix(data=X_train,label=Y_train)
xg_reg = xgb.XGBRegressor()
xg_reg.fit(X_train,Y_train)
y_pred_xg_reg = xg_reg.predict(X_test)
y_pred_xg_reg

# Calculate and return the Mean Absolute Error (MAE) for XGBoost Regressor predictions.
mae_xg_reg = mean_absolute_error(Y_test, y_pred_xg_reg)
mae_xg_reg

"""GRADIENT BOOST"""

# Train a Gradient Boosting Regressor, make predictions, and return the predictions.
gbr = GradientBoostingRegressor()
gbr.fit(X_train, Y_train)
y_pred_gbr= gbr.predict(X_test)
y_pred_gbr

# Calculate and return the Mean Absolute Error (MAE) for Gradient Boosting Regressor predictions.
mae_gbr = mean_absolute_error(Y_test, y_pred_gbr)
mae_gbr

"""CROSS-VALIDATION AND FINETUNING

---


"""

# Setup 3-fold cross-validation, define hyperparameter grid, perform grid search, and fit the model.
#Since the RandomForestRegressor had the lowest MAE, we decided to use it for finetuning.
cv=KFold(n_splits=3)
PARAMETERS ={
'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, 40, 50]
}

model_rf = GridSearchCV(rf, param_grid=PARAMETERS,cv=cv,scoring='neg_mean_absolute_error')
model_rf.fit(X_train,Y_train)

# Find best hyperparameters, train the best RandomForest Regressor, make predictions, and calculate MAE.
best_params=model_rf.best_params_
best_rf_regressor = RandomForestRegressor(**best_params)
best_rf_regressor.fit(X_train, Y_train)
y_pred_best_rf_regressor = best_rf_regressor.predict(X_test)
mae_best_rf_regressor = mean_absolute_error(Y_test, y_pred_best_rf_regressor)
mae_best_rf_regressor

"""TESTING WITH PLAYERS DATA FROM 2022

---


"""

# Assign variables X1 and y1 for feature data and target data, respectively.
X_22=X_22_scaled
y_22=scaled_players22_data['overall']

# Split data into training and testing sets for features (X) and target (Y).
X_train=X_scaled
X_test=X_22_scaled
Y_train=y
Y_test=y_22

# Train a RandomForest Regressor with the best hyperparameters, make predictions, and return the predictions.
rf_22=RandomForestRegressor(**best_params)
rf_22.fit(X_train, Y_train)
y_pred_rf_22 =rf.predict(X_test)
y_pred_rf_22

# Calculate and return the Mean Absolute Error (MAE) for RandomForest Regressor predictions on a new dataset.
mae_rf_22 = mean_absolute_error(Y_test, y_pred_rf_22)
mae_rf_22

"""DEPLOYMENT

---



---


"""

# Save the trained RandomForest Regressor model to a pickle file.
with open('/content/drive/My Drive/Colab Notebooks/model.pkl', 'wb') as file:
    pickle.dump(best_rf_regressor, file)

# Find the average of the overall feature and use that as a true value/target value for finding the confidence level
scaled_players21_data['overall'].mean()

